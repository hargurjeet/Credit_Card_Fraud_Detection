{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hargurjeet/Credit_Card_Fraud_Detection/blob/main/Credit_card_fraud_detection_Starter_code%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn-I9s3wFl3q"
      },
      "source": [
        "## Credit Card Fraud Detection\n",
        "\n",
        "In this project you will predict fraudulent credit card transactions with the help of Machine learning models. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "DeRk0tSeFl3s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMWqCP1wFl3t"
      },
      "source": [
        "## Exploratory data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asgJn9dhFl3t"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('creditcard.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVcudig1Fl3u"
      },
      "outputs": [],
      "source": [
        "#observe the different feature type present in the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg64CSfWFl3u"
      },
      "source": [
        "Here we will observe the distribution of our classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYntE56YFl3u"
      },
      "outputs": [],
      "source": [
        "classes=df['Class'].value_counts()\n",
        "normal_share=classes[0]/df['Class'].count()*100\n",
        "fraud_share=classes[1]/df['Class'].count()*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVXC8Xk2Fl3u"
      },
      "outputs": [],
      "source": [
        "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhbLVj2aFl3v"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot to observe the distribution of classes with time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKbsNWAaFl3v"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot to observe the distribution of classes with Amount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQpHQtHRFl3w"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBAErd-jFl3w"
      },
      "source": [
        "### Splitting the data into train & test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEG0mrwdFl3w"
      },
      "outputs": [],
      "source": [
        "y= #class variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE7Q_An2Fl3x"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "X_train, X_test, y_train, y_test ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyBdGDqoFl3x"
      },
      "source": [
        "##### Preserve X_test & y_test to evaluate on the test data once you build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G66MwYwlFl3x"
      },
      "outputs": [],
      "source": [
        "print(np.sum(y))\n",
        "print(np.sum(y_train))\n",
        "print(np.sum(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4Ydvj0EFl3x"
      },
      "source": [
        "### Plotting the distribution of a variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkUSH1OsFl3y"
      },
      "outputs": [],
      "source": [
        "# plot the histogram of a variable from the dataset to see the skewness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzyjiabwFl3y"
      },
      "source": [
        "### If there is skewness present in the distribution use:\n",
        "- <b>Power Transformer</b> package present in the <b>preprocessing library provided by sklearn</b> to make distribution more gaussian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXfr8fDZFl3y"
      },
      "outputs": [],
      "source": [
        "# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02f3jhSqFl3y"
      },
      "outputs": [],
      "source": [
        "# plot the histogram of a variable from the dataset again to see the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxuRt_4HFl3y"
      },
      "source": [
        "## Model Building\n",
        "- Build different models on the imbalanced dataset and see the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "epKtulODFl3y"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "from sklearn import linear_model #import the package\n",
        "\n",
        "num_C = ______  #--> list of values\n",
        "cv_num =   #--> list of values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2WpnkUXFl3z"
      },
      "source": [
        "#### perfom cross validation on the X_train & y_train to create:\n",
        "- X_train_cv\n",
        "- X_test_cv\n",
        "- y_train_cv\n",
        "- y_test_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVoXcosdFl3z"
      },
      "outputs": [],
      "source": [
        "#perform cross validation\n",
        "\n",
        "#perform hyperparameter tuning\n",
        "\n",
        "#print the evaluation result by choosing a evaluation metric\n",
        "\n",
        "#print the optimum value of hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF214oEnFl3z"
      },
      "source": [
        "### Similarly explore other algorithms by building models like:\n",
        "- KNN\n",
        "- SVM\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_A1D3b5Fl3z"
      },
      "source": [
        "#### Proceed with the model which shows the best result\n",
        "- Apply the best hyperparameter on the model\n",
        "- Predict on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH6znLrAFl3z"
      },
      "outputs": [],
      "source": [
        "clf = ___  #initialise the model with optimum hyperparameters\n",
        "clf.fit(X_train, y_train)\n",
        "print --> #print the evaluation score on the X_test by choosing the best evaluation metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c04vUv3xFl3z"
      },
      "source": [
        "### Print the important features of the best model to understand the dataset\n",
        "- This will not give much explanation on the already transformed dataset\n",
        "- But it will help us in understanding if the dataset is not PCA transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de_K5Z-7Fl3z"
      },
      "outputs": [],
      "source": [
        "var_imp = []\n",
        "for i in clf.feature_importances_:\n",
        "    var_imp.append(i)\n",
        "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
        "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
        "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
        "\n",
        "# Variable on Index-16 and Index-13 seems to be the top 2 variables\n",
        "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
        "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
        "\n",
        "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
        "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
        "\n",
        "np.random.shuffle(X_train_0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "\n",
        "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
        "            label='Actual Class-0 Examples')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6ePrNxFl30"
      },
      "source": [
        "## Model building with balancing Classes\n",
        "\n",
        "##### Perform class balancing with :\n",
        "- Random Oversampling\n",
        "- SMOTE\n",
        "- ADASYN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zukcI9-1Fl30"
      },
      "source": [
        "## Model Building\n",
        "- Build different models on the balanced dataset and see the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3vXbBqTFl30"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "from sklearn import linear_model #import the package\n",
        "\n",
        "num_C = ______  #--> list of values\n",
        "cv_num =   #--> list of values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEKJDohLFl30"
      },
      "source": [
        "#### perfom cross validation on the X_train & y_train to create:\n",
        "- X_train_cv\n",
        "- X_test_cv\n",
        "- y_train_cv\n",
        "- y_test_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpGOq6F_Fl30"
      },
      "source": [
        "### Random Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "S4gzzMlEFl31"
      },
      "outputs": [],
      "source": [
        "from imblearn import over_sampling #- import the packages\n",
        "\n",
        "#perform cross validation & then balance classes on X_train_cv & y_train_cv using Random Oversampling\n",
        "\n",
        "#perform hyperparameter tuning\n",
        "\n",
        "#print the evaluation result by choosing a evaluation metric\n",
        "\n",
        "#print the optimum value of hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yK3NPgSFl31"
      },
      "source": [
        "### Similarly explore other algorithms on balanced dataset by building models like:\n",
        "- KNN\n",
        "- SVM\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuMpnu0JFl31"
      },
      "source": [
        "### Print the class distribution after applying SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm_VAX8TFl31"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "sm = over_sampling.SMOTE(random_state=0)\n",
        "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
        "# Artificial minority samples and corresponding minority labels from SMOTE are appended\n",
        "# below X_train and y_train respectively\n",
        "# So to exclusively get the artificial minority samples from SMOTE, we do\n",
        "X_train_smote_1 = X_train_smote[X_train.shape[0]:]\n",
        "\n",
        "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
        "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
        "\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_smote_1[:X_train_1.shape[0], 0], X_train_smote_1[:X_train_1.shape[0], 1],\n",
        "            label='Artificial SMOTE Class-1 Examples')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT0L8xUuFl4F"
      },
      "outputs": [],
      "source": [
        "#perform cross validation & then balance classes on X_train_cv & y_train_cv using SMOTE\n",
        "\n",
        "#perform hyperparameter tuning\n",
        "\n",
        "#print the evaluation result by choosing a evaluation metric\n",
        "\n",
        "#print the optimum value of hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ybjYj4Fl4F"
      },
      "source": [
        "##### Build models on other algorithms to see the better performing on SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpMI-Fz9Fl4F"
      },
      "source": [
        "### Print the class distribution after applying ADASYN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRVqsSk_Fl4G"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from imblearn import over_sampling\n",
        "\n",
        "ada = over_sampling.ADASYN(random_state=0)\n",
        "X_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train)\n",
        "# Artificial minority samples and corresponding minority labels from ADASYN are appended\n",
        "# below X_train and y_train respectively\n",
        "# So to exclusively get the artificial minority samples from ADASYN, we do\n",
        "X_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:]\n",
        "\n",
        "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
        "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_adasyn_1[:X_train_1.shape[0], 0], X_train_adasyn_1[:X_train_1.shape[0], 1],\n",
        "            label='Artificial ADASYN Class-1 Examples')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26zAv1VdFl4G"
      },
      "outputs": [],
      "source": [
        "#perform cross validation & then balance classes on X_train_cv & y_train_cv using ADASYN\n",
        "\n",
        "#perform hyperparameter tuning\n",
        "\n",
        "#print the evaluation result by choosing a evaluation metric\n",
        "\n",
        "#print the optimum value of hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3PgUWOAFl4G"
      },
      "source": [
        "##### Build models on other algorithms to see the better performing on ADASYN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG1LRwF2Fl4G"
      },
      "source": [
        "### Select the oversampling method which shows the best result on a model\n",
        "- Apply the best hyperparameter on the model\n",
        "- Predict on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUngCh9iFl4G"
      },
      "outputs": [],
      "source": [
        "# perform the best oversampling method on X_train & y_train\n",
        "\n",
        "clf = ___  #initialise the model with optimum hyperparameters\n",
        "clf.fit( ) # fit on the balanced dataset\n",
        "print() --> #print the evaluation score on the X_test by choosing the best evaluation metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fR44HyEFl4H"
      },
      "source": [
        "### Print the important features of the best model to understand the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRzNW_XtFl4H"
      },
      "outputs": [],
      "source": [
        "var_imp = []\n",
        "for i in clf.feature_importances_:\n",
        "    var_imp.append(i)\n",
        "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
        "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
        "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
        "\n",
        "# Variable on Index-13 and Index-9 seems to be the top 2 variables\n",
        "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
        "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
        "\n",
        "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
        "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
        "\n",
        "np.random.shuffle(X_train_0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "\n",
        "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
        "            label='Actual Class-0 Examples')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bME088pJFl4H"
      },
      "outputs": [],
      "source": [
        "#### Print the FPR,TPR & select the best threshold from the roc curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izXCZdLyFl4I"
      },
      "outputs": [],
      "source": [
        "print('Train auc =', metrics.roc_auc_score(_________)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(_________)\n",
        "threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "print(threshold)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}